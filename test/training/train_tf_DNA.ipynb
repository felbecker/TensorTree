{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 10:03:52.466038: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-05 10:03:52.482314: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-05 10:03:52.487349: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-05 10:03:52.500037: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-05 10:03:53.173980: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import tensortree\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_file = \"seq-gen.out\"\n",
    "tree_file = \"RAxML_bestTree.out2\"\n",
    "alphabet = \"ACGT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensortree.set_backend(\"tensorflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse fasta\n",
    "leaf_names = []\n",
    "seqs = []\n",
    "for record in SeqIO.parse(msa_file, \"fasta\"):\n",
    "    leaf_names.append(record.id)\n",
    "    seqs.append(str(record.seq))\n",
    "\n",
    "one_hot_leaves = tensortree.util.encode_one_hot(seqs, alphabet=alphabet)\n",
    "one_hot_leaves = one_hot_leaves[:, np.newaxis] # add model dimension\n",
    "\n",
    "# create tree\n",
    "tree = tensortree.TreeHandler.read(tree_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1733389433.858746  335797 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1733389433.894151  335797 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1733389433.894396  335797 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1733389433.897379  335797 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1733389433.897558  335797 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1733389433.897717  335797 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1733389433.973825  335797 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1733389433.974032  335797 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1733389433.974203  335797 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-12-05 10:03:53.974345: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22299 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "def inverse_softplus(x):\n",
    "    return np.log(np.exp(x) - 1 + 1e-16)\n",
    "\n",
    "# create variables, trivially initialize\n",
    "R, pi = tensortree.substitution_models.jukes_cantor(d = len(alphabet))\n",
    "R_kernel = tf.Variable(inverse_softplus(R))\n",
    "pi_kernel = tf.Variable(np.log(pi))\n",
    "B_kernel = tf.Variable(inverse_softplus(np.ones_like(tree.branch_lengths)))\n",
    "\n",
    "\n",
    "def make_R_pi_B(R_kernel, pi_kernel, B_kernel):\n",
    "    R = tensortree.model.backend.make_symmetric_pos_semidefinite(R_kernel)\n",
    "    pi = tensortree.model.backend.make_equilibrium(pi_kernel)\n",
    "    B = tensortree.model.backend.make_branch_lengths(B_kernel)\n",
    "    return R, pi, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       " array([[-1.        ,  0.33333334,  0.33333334,  0.33333334],\n",
       "        [ 0.33333334, -1.        ,  0.33333334,  0.33333334],\n",
       "        [ 0.33333334,  0.33333334, -1.        ,  0.33333334],\n",
       "        [ 0.33333334,  0.33333334,  0.33333334, -1.        ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0.25, 0.25, 0.25, 0.25]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(7, 1), dtype=float32, numpy=\n",
       " array([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]], dtype=float32)>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check initial values\n",
    "R, pi, B = make_R_pi_B(R_kernel, pi_kernel, B_kernel)\n",
    "Q = tensortree.model.backend.make_rate_matrix(R, pi)\n",
    "Q[0], pi, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function train_step at 0x700ce00fed40> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function train_step at 0x700ce00fed40>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function train_step at 0x700ce00fed40> and will run it as-is.\n",
      "Cause: Unable to locate the source code of <function train_step at 0x700ce00fed40>. Note that functions defined in certain environments, like the interactive Python shell, do not expose their source code. If that is the case, you should define them in a .py source file. If you are certain the code is graph-compatible, wrap the call using @tf.autograph.experimental.do_not_convert. Original error: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-05 10:03:56.665052: I tensorflow/core/util/cuda_solvers.cc:178] Creating GpuSolver handles for stream 0x5a42d04a86c0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after epoch 0=6.580846309661865\n",
      "loss after epoch 1=6.462332725524902\n",
      "loss after epoch 2=6.357039451599121\n",
      "loss after epoch 3=6.262740135192871\n",
      "loss after epoch 4=6.179914951324463\n",
      "loss after epoch 10=5.817017555236816\n",
      "loss after epoch 20=5.374495029449463\n",
      "loss after epoch 30=5.101579189300537\n",
      "loss after epoch 40=4.952778339385986\n",
      "loss after epoch 50=4.876732349395752\n",
      "loss after epoch 60=4.847573280334473\n",
      "loss after epoch 70=4.841607570648193\n",
      "loss after epoch 80=4.838799476623535\n",
      "loss after epoch 90=4.838010787963867\n",
      "CPU times: user 3.83 s, sys: 775 ms, total: 4.6 s\n",
      "Wall time: 4.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# model training\n",
    "lr = 0.05\n",
    "num_steps = 100\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(one_hot_leaves, leaf_names, tree):\n",
    "    with tf.GradientTape() as tape:\n",
    "        R, pi, B = make_R_pi_B(R_kernel, pi_kernel, B_kernel)\n",
    "        Q = tensortree.model.backend.make_rate_matrix(R, pi)\n",
    "        L = tensortree.model.loglik(one_hot_leaves, leaf_names, tree, Q, B, tf.math.log(pi), \n",
    "                            leaves_are_probabilities=True)\n",
    "        loss = -tf.reduce_mean(L)\n",
    "    grads = tape.gradient(loss, [R_kernel, pi_kernel, B_kernel])\n",
    "    return loss, grads\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = lr)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    loss, grads = train_step(one_hot_leaves, leaf_names, tree)\n",
    "    dR, dpi, dB = grads\n",
    "\n",
    "    optimizer.apply_gradients(zip([dR, dpi, dB], [R_kernel, pi_kernel, B_kernel]))\n",
    "\n",
    "    # test output\n",
    "    if step < 5 or step % 10 == 0:\n",
    "        print(f\"loss after epoch {step}={loss.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, pi, B = make_R_pi_B(R_kernel, pi_kernel, B_kernel)\n",
    "Q = tensortree.model.backend.make_rate_matrix(R, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[0.09933323, 0.19983937, 0.30054235, 0.4002851 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       " array([[0.       , 1.8742675, 0.8975281, 1.4307388],\n",
       "        [1.8742675, 0.       , 1.0815687, 1.240525 ],\n",
       "        [0.8975281, 1.0815687, 0.       , 1.6682485],\n",
       "        [1.4307388, 1.240525 , 1.6682485, 0.       ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n",
       " array([[-1.2526371 ,  0.38552007,  0.2776439 ,  0.5894733 ],\n",
       "        [ 0.19162866, -1.0373083 ,  0.33457553,  0.51110405],\n",
       "        [ 0.09176498,  0.22246903, -1.0015628 ,  0.6873288 ],\n",
       "        [ 0.14628145,  0.2551649 ,  0.51606077, -0.9175071 ]],\n",
       "       dtype=float32)>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi, R[0], Q[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.87265918, 0.93632959, 1.49812734, 1.12359551, 1.31086142,\n",
       "        1.68539326]),\n",
       " [0.3745318352059925,\n",
       "  0.28089887640449435,\n",
       "  0.599250936329588,\n",
       "  0.3370786516853932,\n",
       "  0.5243445692883895,\n",
       "  0.6741573033707864])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_pi = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "\n",
    "# normalized rate matrix\n",
    "r = [10, 5, 8, 6, 7, 9]\n",
    "mut = 2 * (true_pi[0] * true_pi[1] * r[0] + true_pi[0] * true_pi[2] * r[1] + true_pi[0] * true_pi[3] * r[2] + \n",
    "           true_pi[1] * true_pi[2] * r[3] + true_pi[1] * true_pi[3] * r[4] + true_pi[2] * true_pi[3] *r[5])\n",
    "rnorm = r / mut\n",
    "q = [rnorm[0] * true_pi[1], rnorm[1] * true_pi[2], rnorm[2] * true_pi[3], \n",
    "     rnorm[3] * true_pi[2], rnorm[4] * true_pi[3], rnorm[5] * true_pi[3]]\n",
    "rnorm, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29544625 0.3017016\n",
      "0.29963386 0.3013877\n",
      "0.19719233 0.2005827\n",
      "0.098468825 0.100148804\n",
      "0.09867688 0.09924722\n",
      "0.3048222 0.29687506\n",
      "0.10516685 0.10143233\n"
     ]
    }
   ],
   "source": [
    "for learned_branch_length, true_branch_length in zip(B[:,0], tree.branch_lengths[:, 0]):\n",
    "    print(learned_branch_length.numpy(), true_branch_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnMSAdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
