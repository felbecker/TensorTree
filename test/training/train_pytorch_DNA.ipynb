{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "import tensortree\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_file = \"seq-gen.out\"\n",
    "tree_file = \"RAxML_bestTree.out2\"\n",
    "alphabet = \"ACGT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensortree.set_backend(\"pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse fasta\n",
    "leaf_names = []\n",
    "seqs = []\n",
    "for record in SeqIO.parse(msa_file, \"fasta\"):\n",
    "    leaf_names.append(record.id)\n",
    "    seqs.append(str(record.seq))\n",
    "\n",
    "one_hot_leaves = tensortree.util.encode_one_hot(seqs, alphabet=alphabet)\n",
    "one_hot_leaves = one_hot_leaves[:, np.newaxis] # add model dimension\n",
    "\n",
    "# create tree\n",
    "tree = tensortree.TreeHandler.read(tree_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_softplus(x):\n",
    "    return np.log(np.exp(x) - 1 + 1e-16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeLogLikelihood(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # create variables, trivially initialize\n",
    "        R, pi = tensortree.substitution_models.jukes_cantor(d = len(alphabet))\n",
    "        self.R_kernel = torch.nn.Parameter(torch.from_numpy(inverse_softplus(R)))\n",
    "        self.pi_kernel = torch.nn.Parameter(torch.from_numpy(np.log(pi)))\n",
    "        self.B_kernel = torch.nn.Parameter(torch.from_numpy(inverse_softplus(np.ones_like(tree.branch_lengths))))\n",
    "\n",
    "\n",
    "    def make_R_pi_B(self):\n",
    "        R = tensortree.model.backend.make_symmetric_pos_semidefinite(self.R_kernel)\n",
    "        pi = tensortree.model.backend.make_equilibrium(self.pi_kernel)\n",
    "        B = tensortree.model.backend.make_branch_lengths(self.B_kernel)\n",
    "        return R, pi, B\n",
    "\n",
    "\n",
    "    def forward(self, one_hot_leaves, leaf_names, tree):\n",
    "        R, pi, B = self.make_R_pi_B()\n",
    "        Q = tensortree.model.backend.make_rate_matrix(R, pi)\n",
    "        L = tensortree.model.loglik(one_hot_leaves, leaf_names, tree, Q, B, torch.log(pi), \n",
    "                            leaves_are_probabilities=True)\n",
    "        loss = -torch.mean(L)\n",
    "        return loss\n",
    "    \n",
    "model = TreeLogLikelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.0000,  0.3333,  0.3333,  0.3333],\n",
       "         [ 0.3333, -1.0000,  0.3333,  0.3333],\n",
       "         [ 0.3333,  0.3333, -1.0000,  0.3333],\n",
       "         [ 0.3333,  0.3333,  0.3333, -1.0000]], grad_fn=<SelectBackward0>),\n",
       " tensor([[0.2500, 0.2500, 0.2500, 0.2500]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [1.]], grad_fn=<SoftplusBackward0>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check initial values\n",
    "R, pi, B = model.make_R_pi_B()\n",
    "Q = tensortree.model.backend.make_rate_matrix(R, pi)\n",
    "Q[0], pi, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/miniforge3/envs/learnMSAdev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after epoch 0=6.581019401550293\n",
      "loss after epoch 1=6.4624924659729\n",
      "loss after epoch 2=6.356645107269287\n",
      "loss after epoch 3=6.262957572937012\n",
      "loss after epoch 4=6.180354595184326\n",
      "loss after epoch 10=5.8167877197265625\n",
      "loss after epoch 20=5.374326229095459\n",
      "loss after epoch 30=5.101367950439453\n",
      "loss after epoch 40=4.95271110534668\n",
      "loss after epoch 50=4.876356601715088\n",
      "loss after epoch 60=4.847850322723389\n",
      "loss after epoch 70=4.841329097747803\n",
      "loss after epoch 80=4.83888053894043\n",
      "loss after epoch 90=4.838085174560547\n",
      "CPU times: user 57.4 s, sys: 5.54 s, total: 1min 2s\n",
      "Wall time: 3.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# model training\n",
    "lr = 0.05\n",
    "num_steps = 100\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for step in range(num_steps):\n",
    "    loss = model(one_hot_leaves, leaf_names, tree)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # test output\n",
    "    if step < 5 or step % 10 == 0:\n",
    "        print(f\"loss after epoch {step}={loss.detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "R, pi, B = model.make_R_pi_B()\n",
    "Q = tensortree.model.backend.make_rate_matrix(R, pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0993, 0.1998, 0.3005, 0.4003]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[0.0000, 1.8744, 0.8975, 1.4309],\n",
       "         [1.8744, 0.0000, 1.0816, 1.2405],\n",
       "         [0.8975, 1.0816, 0.0000, 1.6682],\n",
       "         [1.4309, 1.2405, 1.6682, 0.0000]], grad_fn=<SelectBackward0>),\n",
       " tensor([[-1.2528,  0.3855,  0.2777,  0.5896],\n",
       "         [ 0.1916, -1.0374,  0.3346,  0.5112],\n",
       "         [ 0.0918,  0.2224, -1.0016,  0.6874],\n",
       "         [ 0.1463,  0.2551,  0.5160, -0.9175]], grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi, R[0], Q[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.87265918, 0.93632959, 1.49812734, 1.12359551, 1.31086142,\n",
       "        1.68539326]),\n",
       " [0.3745318352059925,\n",
       "  0.28089887640449435,\n",
       "  0.599250936329588,\n",
       "  0.3370786516853932,\n",
       "  0.5243445692883895,\n",
       "  0.6741573033707864])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_pi = np.array([0.1, 0.2, 0.3, 0.4])\n",
    "\n",
    "# normalized rate matrix\n",
    "r = [10, 5, 8, 6, 7, 9]\n",
    "mut = 2 * (true_pi[0] * true_pi[1] * r[0] + true_pi[0] * true_pi[2] * r[1] + true_pi[0] * true_pi[3] * r[2] + \n",
    "           true_pi[1] * true_pi[2] * r[3] + true_pi[1] * true_pi[3] * r[4] + true_pi[2] * true_pi[3] *r[5])\n",
    "rnorm = r / mut\n",
    "q = [rnorm[0] * true_pi[1], rnorm[1] * true_pi[2], rnorm[2] * true_pi[3], \n",
    "     rnorm[3] * true_pi[2], rnorm[4] * true_pi[3], rnorm[5] * true_pi[3]]\n",
    "rnorm, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.29551047 0.3017016\n",
      "0.29968414 0.3013877\n",
      "0.1972632 0.2005827\n",
      "0.098489024 0.100148804\n",
      "0.09869841 0.09924722\n",
      "0.3048135 0.29687506\n",
      "0.105167374 0.10143233\n"
     ]
    }
   ],
   "source": [
    "for learned_branch_length, true_branch_length in zip(B[:,0], tree.branch_lengths[:, 0]):\n",
    "    print(learned_branch_length.detach().numpy(), true_branch_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learnMSAdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
